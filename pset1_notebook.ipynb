{"cells":[{"cell_type":"markdown","source":"In this programming assignment, we're going to be implementing an MDP, Value Iteration and Policy Iteration, and at Belief State Updates for a POMDP.","metadata":{"tags":[],"cell_id":"00000-1ac3d396-210d-4028-b986-1b86815a10fe","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# MDP formulation","metadata":{"output_cleared":false,"cell_id":"00001-f4e120df-7106-4d80-baca-d24ba89185df","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The first step is to formulate the MDP using a Transition function, T, and a Reward function, R. Use the templates below and the parameters from problem 2 in the written component to implement T and R.","metadata":{"output_cleared":false,"cell_id":"00002-75375e15-b744-4ba0-90ff-64a645c1109c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Transition function","metadata":{"output_cleared":false,"cell_id":"00003-3e1fd4b2-2281-4946-8e76-741b0b292e3e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":false,"cell_id":"00004-c16d8669-7ea3-4b0d-b7bd-895652390892","deepnote_cell_type":"code"},"source":"def T(s, a, s_):\n    \"\"\"Transition function from state s to state s_ via action a.\"\"\"\n    \n    if s == 'healthy':\n        \n        if a == 'vegetables':\n            if s_ == 'healthy':\n                return 1\n            elif s_ == 'sick':\n                return 0\n            elif s_ == 'toothless':\n                return 0\n            else:\n                print('Specify valid state for s_: healthy, sick, or toothless')\n                return\n            \n        elif a == 'candy':\n            if s_ == 'healthy':\n                return 1/4\n            elif s_ == 'sick':\n                return 3/4\n            elif s_ == 'toothless':\n                return 0\n            else:\n                print('Specify valid state for s_: healthy, sick, or toothless')\n                return\n        else:\n            print('Specify valid action for a: candy or vegetables')\n            return\n    \n    elif s == 'sick':\n        if a == 'vegetables':\n            if s_ == 'healthy':\n                return 1/4\n            elif s_ == 'sick':\n                return 3/4\n            elif s_ == 'toothless':\n                return 0\n            else:\n                print('Specify valid state for s_: healthy, sick, or toothless')\n                return\n        elif a == 'candy':\n            if s_ == 'healthy':\n                return 0\n            elif s_ == 'sick':\n                return 7/8\n            elif s_ == 'toothless':\n                return 1/8\n            else:\n                print('Specify valid state for s_: healthy, sick, or toothless')\n                return\n        else:\n            print('Specify valid action for a: candy or vegetables')\n            return\n\n    elif s == 'toothless':\n        if s_ != 'toothless':\n            return 0\n        elif s_ == 'toothless':\n            return 1\n    \n    else:\n        print('Specify valid state for s: healthy, sick, or toothless.')\n        return ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reward function","metadata":{"output_cleared":false,"cell_id":"00005-a0c2acde-bc75-4293-b097-d0a1e1cf29d6","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":false,"cell_id":"00006-8eddb47a-3405-42d7-ad0f-52af3762731f","deepnote_cell_type":"code"},"source":"# STUDENT\ndef R(s, a, s_):\n    if s == 'toothless':\n        return 0\n    elif a == 'candy':\n        return 10\n    elif a == 'vegetables':\n        return 4\n    else:\n        print('Specify valid action for a: candy or vegetables')\n        return","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MDP Solvers","metadata":{"output_cleared":false,"cell_id":"00007-d284ae18-ef37-4086-8b83-4134b9566cf3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Now that you've implemented T and R, implement Value Iteration and Policy Iteration using the templates below and the pseudocode presented in class or in the book.","metadata":{"output_cleared":false,"cell_id":"00008-a85dfd5f-790b-4411-b973-13136085ad0a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Value Iteration","metadata":{"output_cleared":false,"cell_id":"00009-ca961531-a05c-4d76-b1a5-0f604f6120bf","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":false,"cell_id":"00010-4acb175d-37ba-473d-b549-edd386bf8d0f","deepnote_cell_type":"code"},"source":"S = [\"healthy\", \"sick\", \"toothless\"]\nA = [\"candy\", \"vegetables\"]\nepsilon = 0.0001\ngamma = 0.9\n\ndef valueIteration(S, A, T, R, gamma, epsilon):\n    U = {'healthy': 0, 'sick': 0, 'toothless': 0}\n    U_ = {'healthy': 0, 'sick': 0, 'toothless': 0}\n    pi = {}\n    delta = 1\n\n    while gamma * delta > epsilon*(1-gamma):\n        U = dict(U_)\n        delta = 0\n        for state in S:\n            max_u = 0\n            for action in A:\n                u = R(state, action, '')\n                for state_ in S:\n                    u += gamma * T(state, action, state_) * U[state_]\n                if u >= max_u:\n                    max_u = u\n                    pi[state] = action\n            U_[state] = max_u\n            if abs(U_[state] - U[state]) > delta:\n                delta = abs(U_[state] - U[state]) \n\n    if gamma == 0:\n        max_u = 0\n        for action in A:   \n            if R('', action, '') > max_u:\n                max_u = R('', action, '')\n                max_act = action\n        for state in S:\n            pi[state] = max_act\n     \n    return pi \n    # the policy as a dictionary \n    # e.g. pi = {'healthy': 'vegetables', 'sick': 'candy', 'toothless': 'vegetables'}","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Policy Iteration","metadata":{"output_cleared":false,"cell_id":"00011-8f5bfd78-306b-4551-ade1-c00f189f85cc","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":false,"cell_id":"00012-6e6c18eb-d8d7-4be5-b4f6-4940bc2ec247","deepnote_cell_type":"code"},"source":"S = [\"healthy\", \"sick\", \"toothless\"]\nA = [\"candy\", \"vegetables\"]\nepsilon = 0.0001\ngamma = 0.9\n\ndef policyIteration(S, A, T, R, gamma, epsilon):\n    U = {'healthy': 0, 'sick': 0, 'toothless': 0}\n    pi = {'healthy': 'candy', 'sick': 'candy', 'toothless': 'candy'}\n    unchanged = False\n\n    while not unchanged:\n        for state in S:\n            u = R(state, pi[state], '')\n            for state_ in S:\n                u += gamma * T(state, pi[state], state_) * U[state_]\n            U[state] = u\n        unchanged = True\n        for state in S:\n            max_util = U[state]\n            for action in A:\n                u = R(state, action, '')\n                for state_ in S:\n                    u += gamma * T(state, action, state_) * U[state_]\n                if u > max_util:\n                    max_util = u\n                    pi[state] = action\n                    unchanged = False\n    return pi\n    # the policy as a dictionary \n    # e.g. pi = {'healthy': 'vegetables', 'sick': 'candy', 'toothless': 'vegetables'}","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding the $\\gamma$ threshold","metadata":{"output_cleared":false,"cell_id":"00013-3eb23e1a-e5ba-469d-b406-c70ebbf445f3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"You may have noticed that the optimal policy is typically to eat candy in every state. Find the smallest, up to 0.0001 error, discount factor $\\gamma$ such that for $\\pi$, the optimal policy, $\\pi$(sick) = vegetables.","metadata":{"output_cleared":false,"cell_id":"00014-8ea779fe-0b57-4b61-8313-80cf32bbeac1","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":false,"cell_id":"00015-00f8864c-4516-45b2-a599-ea8d8e41b333","deepnote_cell_type":"code"},"source":"def find_gamma(solver, epsilon=0.0001):\n    gamma = 1\n    pi = solver(S, A, T, R, 0.9, epsilon)\n    while pi['sick'] == 'vegetables':\n        gamma -= 0.1\n        pi = solver(S, A, T, R, gamma, epsilon)\n    while pi['sick'] != 'vegetables':\n        gamma += 0.01\n        pi = solver(S, A, T, R, gamma, epsilon)\n    while pi['sick'] == 'vegetables':\n        gamma -= 0.001\n        pi = solver(S, A, T, R, gamma, epsilon)\n    while pi['sick'] != 'vegetables':\n        gamma += 0.0001\n        pi = solver(S, A, T, R, gamma, epsilon)\n    gamma_min = gamma    \n    \n    return gamma_min","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# POMDP Belief States","metadata":{"output_cleared":false,"cell_id":"00016-92e28392-2f18-45bf-9c19-dd7c2c650783","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"In problem 3 of the written you computed belief state updates by hand. To do this you had to compute the evidence posterior $\\mathbb P(e | s)$ and run the FORWARD updates $$b(s') = \\alpha \\mathbb P(e|s') \\sum_{s \\in S} \\mathbb P(s' | s, a) b(s).$$\nImplement the evidence posterior, O, below and the update_belief_state method using the FORWARD update. You can use this to check your answer for problem 3(c) in the written.","metadata":{"output_cleared":false,"cell_id":"00017-47f5b529-46fd-4c93-a825-f3185c170540","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Evidence posterior","metadata":{"output_cleared":false,"cell_id":"00018-a574fdc7-45e4-4e95-89d8-1d483f3611cd","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":false,"cell_id":"00019-f3e68525-d513-487e-8979-da798ea07fac","deepnote_cell_type":"code"},"source":"def O(evidence, state):\n    \"\"\"Returns probably of observing evidence ('F' if toothless, 0 if T < 100, 1 if T >= 100) given state.\"\"\"\n\n    if evidence == 'F':\n        if state == 'toothless':\n            return 1\n        else:\n            return 0\n    else:\n        temperature = evidence\n        # State Posteriors\n        P_healthy_given_T_minus = 0.9\n        P_sick_given_T_minus = 0.1\n        \n        # Evidence Posteriors\n        P_healthy_given_T_plus = 0.01 # FILL IN\n        P_sick_given_T_plus = 0.99 # FILL IN\n        \n        # Evidence Priors\n        P_T_minus = 0.9438 # FILL IN\n        P_T_plus = 0.0562 # FILL IN\n        \n        # State Priors\n        P_healthy = 0.85 # FILL IN\n        P_sick = 0.15 # FILL IN\n\n        if temperature == 0:\n            \n            P_temperature = P_T_minus\n            \n            if state == 'healthy':\n                P_state_given_temperature = P_healthy_given_T_minus\n                P_state = P_healthy\n            elif state == 'sick':\n                P_state_given_temperature = P_sick_given_T_minus\n                P_state = P_sick\n            elif state == 'toothless':\n                P_state_given_temperature = 0\n                P_state = 0.01\n            else:\n                print('Specify healthy or sick for state!')\n                return\n            \n        elif temperature == 1:\n            \n            P_temperature = P_T_plus\n            \n            if state == 'healthy':\n                P_state_given_temperature = P_healthy_given_T_plus\n                P_state = P_healthy\n            elif state == 'sick':\n                P_state_given_temperature = P_sick_given_T_plus\n                P_state = P_sick\n            elif state == 'toothless':\n                P_state_given_temperature = 0\n                P_state = 0.01\n            else:\n                print('Specify healthy or sick for state!')\n                return\n\n        else:\n            print('Specify binary variable for temperature!')\n            return\n        \n        P_temperature_given_state = P_state_given_temperature * P_temperature / P_state # FILL IN\n        \n        return P_temperature_given_state # Update probability computed with Bayes' rule","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Belief state update ","metadata":{"output_cleared":false,"cell_id":"00020-2f4a68c2-51ba-4e92-a6d2-fc2c8ecac937","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":false,"cell_id":"00021-dd716c91-fbe4-48d4-804e-d309d5e96763","deepnote_cell_type":"code"},"source":"def update_belief_state(temperature, action, belief_state):\n    s = ['healthy', 'sick', 'toothless']\n    b_list = []\n    for i in range(0, len(belief_state)):\n        b_ = 0\n        for j in range(0, len(belief_state)):\n            b_ += T(s[j], action, s[i]) * belief_state[j]\n        b_list.append(b_ * O(temperature, s[i]))\n    b_0 = b_list[0] / sum(b_list)\n    b_1 = b_list[1] / sum(b_list)\n    b_2 = b_list[2] / sum(b_list)\n\n    updated_belief_state = (b_0, b_1, b_2)\n    return updated_belief_state \n    # 3-tuple (b_0', b_1', b_2') representing updated probabilities of each state {\"healthy\", \"sick\", \"toothless\"}","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-a2a8e508-fffa-4519-964a-d3236bf69901","output_cleared":false,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"},"deepnote_notebook_id":"0443bf03-42a7-4054-83d8-b783d16320b9","deepnote_execution_queue":[]}}